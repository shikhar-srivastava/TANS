{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Zoo Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== Testing dataset:  recognizance1_0_5\n",
      "dict_keys(['task', 'clss', 'nclss', 'x_train', 'y_train', 'x_test', 'y_test', 'query'])\n",
      "1324 334 1324 334\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([50, 512])\n",
      "====================================================\n",
      "========================== Testing dataset:  gemstones-images_lsind18_18_36\n",
      "dict_keys(['task', 'clss', 'nclss', 'x_train', 'y_train', 'x_test', 'y_test', 'query'])\n",
      "432 119 432 119\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([180, 512])\n",
      "====================================================\n",
      "========================== Testing dataset:  drr-sign_0_4\n",
      "dict_keys(['task', 'clss', 'nclss', 'x_train', 'y_train', 'x_test', 'y_test', 'query'])\n",
      "272 71 272 71\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([40, 512])\n",
      "====================================================\n",
      "========================== Testing dataset:  ucfai-core-fa19-cnns_76_95\n",
      "dict_keys(['task', 'clss', 'nclss', 'x_train', 'y_train', 'x_test', 'y_test', 'query'])\n",
      "754 197 754 197\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([190, 512])\n",
      "====================================================\n",
      "========================== Testing dataset:  honey-bee-pollen_ivanfel_0_2\n",
      "dict_keys(['task', 'clss', 'nclss', 'x_train', 'y_train', 'x_test', 'y_test', 'query'])\n",
      "571 143 571 143\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([20, 512])\n",
      "====================================================\n",
      "========================== Testing dataset:  alien-vs-predator-images_pmigdal_0_2\n",
      "dict_keys(['task', 'clss', 'nclss', 'x_train', 'y_train', 'x_test', 'y_test', 'query'])\n",
      "711 179 711 179\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([20, 512])\n",
      "====================================================\n",
      "========================== Testing dataset:  kuzushiji_anokas_1120_1140\n",
      "dict_keys(['task', 'clss', 'nclss', 'x_train', 'y_train', 'x_test', 'y_test', 'query'])\n",
      "940 245 940 245\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([192, 512])\n",
      "====================================================\n",
      "========================== Testing dataset:  covid19-radiography-database_tawsifurrahman_0_3\n",
      "dict_keys(['task', 'clss', 'nclss', 'x_train', 'y_train', 'x_test', 'y_test', 'query'])\n",
      "2298 576 2298 576\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([30, 512])\n",
      "====================================================\n",
      "========================== Testing dataset:  colorectal-histology-mnist_kmader_0_8\n",
      "dict_keys(['task', 'clss', 'nclss', 'x_train', 'y_train', 'x_test', 'y_test', 'query'])\n",
      "4000 1000 4000 1000\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([80, 512])\n",
      "====================================================\n",
      "========================== Testing dataset:  ml2020spring-hw12_0_10\n",
      "dict_keys(['task', 'clss', 'nclss', 'x_train', 'y_train', 'x_test', 'y_test', 'query'])\n",
      "4000 1000 4000 1000\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([100, 512])\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "path = '/nfs/projects/mbzuai/shikhar/datasets/ofa/data_path/meta_test_'\n",
    "test_datasets = [\n",
    "                'recognizance1_0_5',\n",
    "                'gemstones-images_lsind18_18_36',\n",
    "                'drr-sign_0_4',\n",
    "                'ucfai-core-fa19-cnns_76_95',\n",
    "                'honey-bee-pollen_ivanfel_0_2',\n",
    "                'alien-vs-predator-images_pmigdal_0_2',\n",
    "                'kuzushiji_anokas_1120_1140',\n",
    "                'covid19-radiography-database_tawsifurrahman_0_3',\n",
    "                'colorectal-histology-mnist_kmader_0_8',\n",
    "                'ml2020spring-hw12_0_10',\n",
    "        ]\n",
    "import torch\n",
    "for dataset in test_datasets:\n",
    "    print('========================== Testing dataset: ', dataset)\n",
    "    data_ = torch.load(path + dataset + '.pt')\n",
    "    print(data_.keys())\n",
    "    print(len(data_['x_train']), len(data_['x_test']), len(data_['y_train']), len(data_['y_test']))\n",
    "    print(data_['x_train'][0].shape)\n",
    "    print(data_['query'].shape)\n",
    "    print('====================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_['x_train'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def gen_leave4out():\n",
    "    MRI = [\"Brain_MRI\",\"ProstateMRI\"]\n",
    "    XRAY = [\"RSNAXRay\",\"Covid19XRay\"]\n",
    "    CT = [ \"MosMed\",\n",
    "    \"kits\",\n",
    "    \"LiTs\",\n",
    "    \"RSPECT\",\n",
    "    \"IHD_Brain\",\n",
    "    \"ImageCHD\",\n",
    "    \"CTPancreas\"]\n",
    "    # get all combinations of 2 CT datasets with 1 MRI and 1 XRAY\n",
    "    combs = []\n",
    "    for mri in MRI:\n",
    "        for xray in XRAY:\n",
    "            for (ct1,ct2) in combinations(CT, 2):\n",
    "                combs.append([ct1,ct2,mri,xray])\n",
    "    return combs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gen_leave4out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Test DatasetEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [05:03<00:00, 25.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from meta.embedding import DatasetEmbeddings\n",
    "import torch\n",
    "data_embedding = DatasetEmbeddings()\n",
    "data_dict = data_embedding.meta_test_embed_datasets(n_samples = 200)\n",
    "# save torch \n",
    "data_embedding_path = '/nfs/projects/mbzuai/shikhar/datasets/ofa/our_data_path/meta_test_all.pt'\n",
    "# save data_dict to path\n",
    "torch.save(data_dict,data_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RSPECT': {'task': 'RSPECT',\n",
       "  'clss': {0, 1},\n",
       "  'nclss': 2,\n",
       "  'query': tensor([[0.7098, 0.4983, 0.4113,  ..., 1.7854, 0.2189, 0.0347],\n",
       "          [1.3022, 0.1384, 0.2035,  ..., 0.2683, 0.3346, 0.1329],\n",
       "          [0.5595, 0.2527, 0.7823,  ..., 2.5134, 0.0397, 0.0719],\n",
       "          ...,\n",
       "          [0.4306, 0.4859, 0.5044,  ..., 0.6061, 0.1808, 0.2323],\n",
       "          [0.8418, 0.4605, 0.3567,  ..., 1.4755, 0.1574, 0.1054],\n",
       "          [0.1616, 0.3550, 0.5746,  ..., 1.7063, 0.0903, 0.1980]])},\n",
       " 'kits': {'task': 'kits',\n",
       "  'clss': {0, 1},\n",
       "  'nclss': 2,\n",
       "  'query': tensor([[4.9051e+02, 4.1101e+02, 7.5922e+01,  ..., 0.0000e+00, 5.2010e+00,\n",
       "           6.3538e+01],\n",
       "          [1.2865e+03, 6.0080e+02, 1.5994e+02,  ..., 5.1198e+00, 0.0000e+00,\n",
       "           2.2572e+02],\n",
       "          [6.1376e+02, 4.0675e+02, 7.1212e+01,  ..., 0.0000e+00, 2.2764e+01,\n",
       "           1.1065e+02],\n",
       "          ...,\n",
       "          [8.2254e+02, 5.0486e+02, 1.4378e+02,  ..., 0.0000e+00, 4.1941e-01,\n",
       "           1.4506e+02],\n",
       "          [1.0161e+03, 5.9235e+02, 2.7489e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           1.2846e+02],\n",
       "          [7.4707e+02, 3.8266e+02, 1.2655e+02,  ..., 1.9490e-01, 1.0781e+01,\n",
       "           1.7937e+02]])},\n",
       " 'ProstateMRI': {'task': 'ProstateMRI',\n",
       "  'clss': {0, 1},\n",
       "  'nclss': 2,\n",
       "  'query': tensor([[163.3707,  89.6449,   2.0937,  ...,   7.0642,   0.0000,   0.0000],\n",
       "          [ 66.4827,  62.5422,   1.8571,  ...,   1.2228,   0.0000,   0.0000],\n",
       "          [ 97.1760,  86.6864,   2.7097,  ...,  12.9023,   0.0000,   0.0000],\n",
       "          ...,\n",
       "          [ 68.5358,  66.1595,   0.2091,  ...,   5.0860,   0.0000,   2.4264],\n",
       "          [100.2765,  14.4618,   2.9268,  ...,   0.0000,  25.3746,   0.7402],\n",
       "          [ 19.3819,  20.2378,   2.6865,  ...,   1.3336,   0.0000,   1.8825]])},\n",
       " 'CTPancreas': {'task': 'CTPancreas',\n",
       "  'clss': {0, 1},\n",
       "  'nclss': 2,\n",
       "  'query': tensor([[1.0958e+03, 6.6009e+02, 1.5332e+02,  ..., 2.2003e-01, 0.0000e+00,\n",
       "           2.5991e+02],\n",
       "          [9.3751e+02, 5.5535e+02, 2.0155e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           1.0280e+02],\n",
       "          [1.4682e+03, 7.3177e+02, 1.6026e+02,  ..., 1.3524e-01, 0.0000e+00,\n",
       "           2.0135e+02],\n",
       "          ...,\n",
       "          [8.6883e+02, 5.9353e+02, 1.2509e+02,  ..., 0.0000e+00, 1.1028e+01,\n",
       "           8.7744e+01],\n",
       "          [9.6610e+02, 6.9059e+02, 1.9034e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           1.7497e+02],\n",
       "          [9.0150e+02, 4.6593e+02, 1.1258e+02,  ..., 0.0000e+00, 1.8242e+01,\n",
       "           8.8961e+01]])},\n",
       " 'IHD_Brain': {'task': 'IHD_Brain',\n",
       "  'clss': {0, 1},\n",
       "  'nclss': 2,\n",
       "  'query': tensor([[614.3785, 435.7519,  94.5051,  ...,   1.0401,   0.0000, 166.9321],\n",
       "          [123.3771, 245.4089,  20.2718,  ...,  18.0812,   0.0000,   1.3628],\n",
       "          [200.4189, 345.3267,  24.6430,  ...,  10.7914,   0.0000,   2.6856],\n",
       "          ...,\n",
       "          [202.1472,  76.3906,  38.3544,  ...,   0.0000,  33.1794,  10.7465],\n",
       "          [169.2156, 297.9937,  24.2032,  ...,  14.9770,   0.0000,  25.6499],\n",
       "          [173.6282,  92.1700,  13.8526,  ...,  37.0653,   0.0000,   2.8936]])},\n",
       " 'RSNAXRay': {'task': 'RSNAXRay',\n",
       "  'clss': {0, 1},\n",
       "  'nclss': 2,\n",
       "  'query': tensor([[16.6817, 10.9320,  0.0437,  ...,  0.9591,  0.0000,  0.0000],\n",
       "          [12.4993,  6.7989,  0.0361,  ...,  2.1116,  0.0000,  0.0000],\n",
       "          [16.3036,  7.0779,  0.5046,  ...,  6.7942,  0.1549,  7.0993],\n",
       "          ...,\n",
       "          [33.5484, 19.9991,  0.3225,  ...,  0.2104, 10.9066, 11.0403],\n",
       "          [17.1431, 12.2625,  0.1004,  ...,  4.9213,  0.0000,  0.0000],\n",
       "          [21.8102, 12.8874,  0.0624,  ...,  5.3480,  0.0000,  0.0000]])},\n",
       " 'MosMed': {'task': 'MosMed',\n",
       "  'clss': {0, 1},\n",
       "  'nclss': 2,\n",
       "  'query': tensor([[1124.9896,  678.5895,  106.6166,  ...,    0.0000,    0.0000,\n",
       "             81.7568],\n",
       "          [ 934.8710,  746.7728,   66.7911,  ...,    0.0000,    0.0000,\n",
       "            105.2157],\n",
       "          [ 968.9539,  741.5432,   75.4908,  ...,    0.0000,    0.0000,\n",
       "            166.8214],\n",
       "          ...,\n",
       "          [1006.2094,  457.1959,  148.1438,  ...,    0.0000,    0.0000,\n",
       "            165.3742],\n",
       "          [ 888.3307,  449.9373,  124.5170,  ...,    0.0000,   33.3343,\n",
       "            123.8435],\n",
       "          [ 743.8755,  562.4092,  161.3621,  ...,    0.0000,    0.0000,\n",
       "             27.0944]])},\n",
       " 'Covid19XRay': {'task': 'Covid19XRay',\n",
       "  'clss': {0, 1, 2, 3},\n",
       "  'nclss': 4,\n",
       "  'query': tensor([[2.1929e+02, 2.3730e+02, 0.0000e+00,  ..., 1.4835e+01, 0.0000e+00,\n",
       "           2.6052e+00],\n",
       "          [1.9010e+02, 1.9438e+02, 3.6925e+00,  ..., 3.2244e+01, 0.0000e+00,\n",
       "           0.0000e+00],\n",
       "          [2.0899e+02, 1.4573e+02, 1.5496e+00,  ..., 2.9724e+01, 0.0000e+00,\n",
       "           0.0000e+00],\n",
       "          ...,\n",
       "          [3.0947e+02, 1.9898e+02, 1.3222e-01,  ..., 5.1612e+01, 0.0000e+00,\n",
       "           0.0000e+00],\n",
       "          [2.7602e+02, 2.3931e+02, 1.1910e+00,  ..., 5.5312e+01, 0.0000e+00,\n",
       "           3.7909e+00],\n",
       "          [1.9843e+02, 2.3560e+02, 5.0277e+00,  ..., 3.1646e+01, 0.0000e+00,\n",
       "           0.0000e+00]])},\n",
       " 'Brain_MRI': {'task': 'Brain_MRI',\n",
       "  'clss': {0, 1, 2, 3},\n",
       "  'nclss': 4,\n",
       "  'query': tensor([[6.7389e+00, 5.1367e+00, 3.3232e+00,  ..., 0.0000e+00, 2.2512e+00,\n",
       "           2.2164e-01],\n",
       "          [2.6647e+01, 1.0310e+00, 3.4098e+00,  ..., 0.0000e+00, 8.7764e+00,\n",
       "           5.5194e-02],\n",
       "          [2.6083e+01, 4.0802e+00, 1.7287e+00,  ..., 4.3860e-02, 2.7986e+00,\n",
       "           1.8659e+00],\n",
       "          ...,\n",
       "          [7.8055e+00, 4.1956e+00, 4.1566e+00,  ..., 1.8680e-02, 9.2925e-01,\n",
       "           1.1496e-01],\n",
       "          [9.8357e+00, 6.7908e+00, 2.9764e+00,  ..., 2.4402e-02, 3.1194e+00,\n",
       "           1.0993e+00],\n",
       "          [1.5343e+01, 1.0689e+01, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           1.9728e+00]])},\n",
       " 'fetal_ultrasound': {'task': 'fetal_ultrasound',\n",
       "  'clss': {0, 1, 2, 3, 4},\n",
       "  'nclss': 5,\n",
       "  'query': tensor([[1.3516e+01, 2.2013e+01, 1.2335e+00,  ..., 3.1659e-02, 0.0000e+00,\n",
       "           2.3725e+00],\n",
       "          [3.4188e+01, 5.9048e+00, 1.8516e+00,  ..., 0.0000e+00, 1.2530e+00,\n",
       "           3.1226e+00],\n",
       "          [1.9490e+01, 4.9162e+00, 1.0453e+00,  ..., 0.0000e+00, 3.4436e+00,\n",
       "           4.3682e-01],\n",
       "          ...,\n",
       "          [9.9275e+00, 6.7717e+00, 1.5464e+00,  ..., 1.6351e-01, 2.9676e+00,\n",
       "           2.7735e-01],\n",
       "          [3.1309e+01, 9.9267e-01, 1.9151e+00,  ..., 0.0000e+00, 8.0747e+00,\n",
       "           4.1980e-01],\n",
       "          [2.0717e+01, 4.9894e+00, 2.1133e+00,  ..., 0.0000e+00, 4.6604e+01,\n",
       "           3.8617e-01]])},\n",
       " 'ImageCHD': {'task': 'ImageCHD',\n",
       "  'clss': {0, 1},\n",
       "  'nclss': 2,\n",
       "  'query': tensor([[252.0026, 158.9091,  76.8325,  ...,   9.4540,   7.9426,  20.8462],\n",
       "          [287.4868, 145.0658,  29.9571,  ...,  11.8329,   5.4618,  38.3997],\n",
       "          [196.3490, 161.6833,   3.4024,  ...,  45.7715,   0.0000,   0.0000],\n",
       "          ...,\n",
       "          [133.1383, 133.4259,  49.0245,  ...,   0.0000,   0.6686,   5.9662],\n",
       "          [206.4285, 118.8567,  83.0228,  ...,   0.0000,   0.0000,   3.4570],\n",
       "          [199.7859, 109.0660,  39.2920,  ...,  45.9041,   1.9647,  24.7044]])},\n",
       " 'LiTs': {'task': 'LiTs',\n",
       "  'clss': {0, 1},\n",
       "  'nclss': 2,\n",
       "  'query': tensor([[8.2493e+02, 5.0211e+02, 1.6375e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           8.7178e+01],\n",
       "          [7.4554e+02, 3.8542e+02, 6.7531e+01,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           1.0013e+02],\n",
       "          [1.1277e+03, 6.3050e+02, 1.2300e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           1.3595e+02],\n",
       "          ...,\n",
       "          [8.4639e+02, 2.5173e+02, 1.4566e+02,  ..., 0.0000e+00, 7.0411e-01,\n",
       "           1.1467e+02],\n",
       "          [9.1295e+02, 5.0450e+02, 1.3430e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           2.1739e+02],\n",
       "          [5.6494e+02, 3.9330e+02, 8.3891e+01,  ..., 0.0000e+00, 5.0796e+01,\n",
       "           1.9218e+02]])}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Embeddings for Meta Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "# Read pt file from path \n",
    "#path = '/l/users/shikhar.srivastava/data/ofa/www.dropbox.com/s/1qbsgjqanxgw2ji/p_m_train.pt'\n",
    "path ='/nfs/projects/mbzuai/shikhar/datasets/ofa/p_m_train.pt' #p_mod_zoo.pt\n",
    "data_files = torch.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['6000-store-items-images-classified-by-color_imoore_0_12', 'ads5035-01_0_2', 'ai2020f_0_11', 'aia-dl-mid_0_4', 'apparel-images-dataset_trolukovich_0_12', 'apparel-images-dataset_trolukovich_12_24', 'asl-alphabet_grassknoted_0_15', 'asl-alphabet_grassknoted_15_29', 'best-artworks-of-all-time_ikarus777_0_17', 'best-artworks-of-all-time_ikarus777_17_34', 'best-artworks-of-all-time_ikarus777_34_51', 'blood-cells_paultimothymooney_0_4', 'breakhis_ambarish_0_8', 'breast-histopathology-images_paultimothymooney_0_2', 'cactus-aerial-photos_irvingvasquez_0_2', 'car-classificationproject-vision_0_15', 'car-classificationproject-vision_15_30', 'car-classificationproject-vision_30_45', 'cassava-leaf-disease-classification_0_5', 'celeba-dataset_jessicali9530_0_2', 'chessman-image-dataset_niteshfre_0_6', 'classification-of-handwritten-letters_olgabelitskaya_0_17', 'classification-of-handwritten-letters_olgabelitskaya_17_33', 'computed-tomography-ct-images_vbookshelf_0_2', 'corales_0_14', 'crack-identification-ce784a-2020-iitk_0_2', 'cs4487-2020fall_0_2', 'cs4670spring2020pa3_0_16', 'csep546-aut19-kc2_0_5', 'cv2020-classification-challenge_0_20', 'cv2020-classification-challenge_100_120', 'cv2020-classification-challenge_20_40', 'cv2020-classification-challenge_40_60', 'cv2020-classification-challenge_60_80', 'cv2020-classification-challenge_80_100', 'day-3-kaggle-competition_0_5', 'defi1-ia_0_3', 'devanagari-character-set_rishianand_0_16', 'devanagari-character-set_rishianand_16_31', 'devanagari-character-set_rishianand_31_46', 'DL2020_0_4', 'dlai3_0_2', 'e4040fall2019-assignment-2-task-5_0_5', 'fcis-sc-deeplearning-competition_0_10', 'flowers-recognition_alxmamaev_0_5', 'four-shapes_smeschke_0_4', 'fruit-recognition_chrisfilo_0_15', 'fruits_moltean_0_19', 'fruits_moltean_113_131', 'fruits_moltean_19_38', 'fruits_moltean_38_57', 'fruits_moltean_57_76', 'fruits_moltean_76_95', 'fruits_moltean_95_113', 'garbage-classification_asdasdasasdas_0_6', 'gen-2-ai-force-challenge-1_0_10', 'gpa759-2020_0_17', 'gpa759-2020_17_34', 'gpa759-2020_34_50', 'gtsrb-german-traffic-sign_meowmeowmeowmeowmeow_0_15', 'gtsrb-german-traffic-sign_meowmeowmeowmeowmeow_15_29', 'gtsrb-german-traffic-sign_meowmeowmeowmeowmeow_29_43', 'hackathon-blossom-flower-classification_spaics_0_17', 'hackathon-blossom-flower-classification_spaics_17_34', 'hackathon-blossom-flower-classification_spaics_34_51', 'hackathon-blossom-flower-classification_spaics_51_68', 'hackathon-blossom-flower-classification_spaics_68_85', 'hackathon-blossom-flower-classification_spaics_85_102', 'image-classification_duttadebadri_0_4', 'intel-image-classification_puneet6060_0_6', 'khu-deep-learning-competition_0_10', 'kunstmatigeintelligentie20192020_0_5', 'labeled-surgical-tools_dilavado_0_4', 'land-cover-class_0_10', 'lego-brick-images_joosthazelzet_0_16', 'lego-brick-sorting-image-recognition_pacogarciam3_0_20', 'lego-minifigures-classification_ihelon_0_14', 'lego-vs-generic-brick-image-recognition_pacogarciam3_0_4', 'make-up-vs-no-make-up_petersunga_0_2', 'malefemale-for-drr_0_2', 'messy-vs-clean-room_cdawn1_0_2', 'microsoft-catsvsdogs-dataset_shaunthesheep_0_2', 'mis583-hw2-part-2_0_5', 'mllabgame_0_10', 'mushrooms-classification-common-genuss-images_maysee_0_9', 'mvtec_anomaly_detection_carpet_0_6', 'mvtec_anomaly_detection_grid_0_6', 'mvtec_anomaly_detection_leather_0_6', 'nnfl-cnn-lab2_0_6', 'notmnist_jwjohnson314_0_10', 'numta_BengaliAI_0_10', 'nuu-me-midterm-exam-image-classification_0_5', 'oregon-wildlife_virtualdvid_0_20', 'parkinsons-drawings_kmader_0_2', 'perritos_0_10', 'plant-seedlings-classification_0_12', 'proptit-aif-homework-1_0_8', 'real-and-fake-face-detection_ciplab_0_2', 'real-life-industrial-dataset-of-casting-product_ravirajsinh45_0_2', 'rockpaperscissors_drgfreeman_0_3', 'sample_nih-chest-xrays_0_2', 'sfu-cmpt-computer-vision-course-cnn_0_20', 'sfu-cmpt-computer-vision-course-cnn_100_120', 'sfu-cmpt-computer-vision-course-cnn_120_140', 'sfu-cmpt-computer-vision-course-cnn_140_160', 'sfu-cmpt-computer-vision-course-cnn_160_180', 'sfu-cmpt-computer-vision-course-cnn_180_200', 'simpsons4_0_20', 'sfu-cmpt-computer-vision-course-cnn_20_40', 'sfu-cmpt-computer-vision-course-cnn_40_60', 'sfu-cmpt-computer-vision-course-cnn_60_80', 'sfu-cmpt-computer-vision-course-cnn_80_100', 'sheep-breed-classification_divyansh22_0_4', 'simpsons4_20_39', 'simpsons-challenge-gft_0_20', 'simpsons-challenge-gft_20_39', 'skin-cancer9-classesisic_nodoubttome_0_9', 'sldc_0_10', 'stanford-dogs-dataset_jessicali9530_0_20', 'stanford-dogs-dataset_jessicali9530_100_120', 'stanford-dogs-dataset_jessicali9530_20_40', 'stanford-dogs-dataset_jessicali9530_40_60', 'stanford-dogs-dataset_jessicali9530_60_80', 'stanford-dogs-dataset_jessicali9530_80_100', 'stanford-dogs-dataset-traintest_miljan_0_20', 'stanford-dogs-dataset-traintest_miljan_100_120', 'stanford-dogs-dataset-traintest_miljan_20_40', 'stanford-dogs-dataset-traintest_miljan_40_60', 'stanford-dogs-dataset-traintest_miljan_60_80', 'stanford-dogs-dataset-traintest_miljan_80_100', 'synthetic-digits_prasunroy_0_10', 'tau-ethiopic-digit-recognition_0_10', 'the-simpsons-characters-dataset_alexattia_0_20', 'the-simpsons-characters-dataset_alexattia_20_39', 'tl-signs-hse-itmo-2020-winter_0_17', 'tl-signs-hse-itmo-2020-winter_17_34', 'tl-signs-hse-itmo-2020-winter_34_51', 'tl-signs-hse-itmo-2020-winter_51_67', 'vehicle_0_17', 'zalando-store-crawl_dqmonn_0_6'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['6000-store-items-images-classified-by-color_imoore_0_12', 'ads5035-01_0_2', 'ai2020f_0_11', 'aia-dl-mid_0_4', 'apparel-images-dataset_trolukovich_0_12', 'apparel-images-dataset_trolukovich_12_24', 'asl-alphabet_grassknoted_0_15', 'asl-alphabet_grassknoted_15_29', 'best-artworks-of-all-time_ikarus777_0_17', 'best-artworks-of-all-time_ikarus777_17_34', 'best-artworks-of-all-time_ikarus777_34_51', 'blood-cells_paultimothymooney_0_4', 'breakhis_ambarish_0_8', 'breast-histopathology-images_paultimothymooney_0_2', 'cactus-aerial-photos_irvingvasquez_0_2', 'car-classificationproject-vision_0_15', 'car-classificationproject-vision_15_30', 'car-classificationproject-vision_30_45', 'cassava-leaf-disease-classification_0_5', 'celeba-dataset_jessicali9530_0_2', 'chessman-image-dataset_niteshfre_0_6', 'classification-of-handwritten-letters_olgabelitskaya_0_17', 'classification-of-handwritten-letters_olgabelitskaya_17_33', 'computed-tomography-ct-images_vbookshelf_0_2', 'corales_0_14', 'crack-identification-ce784a-2020-iitk_0_2', 'cs4487-2020fall_0_2', 'cs4670spring2020pa3_0_16', 'csep546-aut19-kc2_0_5', 'cv2020-classification-challenge_0_20', 'cv2020-classification-challenge_100_120', 'cv2020-classification-challenge_20_40', 'cv2020-classification-challenge_40_60', 'cv2020-classification-challenge_60_80', 'cv2020-classification-challenge_80_100', 'day-3-kaggle-competition_0_5', 'defi1-ia_0_3', 'devanagari-character-set_rishianand_0_16', 'devanagari-character-set_rishianand_16_31', 'devanagari-character-set_rishianand_31_46', 'DL2020_0_4', 'dlai3_0_2', 'e4040fall2019-assignment-2-task-5_0_5', 'fcis-sc-deeplearning-competition_0_10', 'flowers-recognition_alxmamaev_0_5', 'four-shapes_smeschke_0_4', 'fruit-recognition_chrisfilo_0_15', 'fruits_moltean_0_19', 'fruits_moltean_113_131', 'fruits_moltean_19_38', 'fruits_moltean_38_57', 'fruits_moltean_57_76', 'fruits_moltean_76_95', 'fruits_moltean_95_113', 'garbage-classification_asdasdasasdas_0_6', 'gen-2-ai-force-challenge-1_0_10', 'gpa759-2020_0_17', 'gpa759-2020_17_34', 'gpa759-2020_34_50', 'gtsrb-german-traffic-sign_meowmeowmeowmeowmeow_0_15', 'gtsrb-german-traffic-sign_meowmeowmeowmeowmeow_15_29', 'gtsrb-german-traffic-sign_meowmeowmeowmeowmeow_29_43', 'hackathon-blossom-flower-classification_spaics_0_17', 'hackathon-blossom-flower-classification_spaics_17_34', 'hackathon-blossom-flower-classification_spaics_34_51', 'hackathon-blossom-flower-classification_spaics_51_68', 'hackathon-blossom-flower-classification_spaics_68_85', 'hackathon-blossom-flower-classification_spaics_85_102', 'image-classification_duttadebadri_0_4', 'intel-image-classification_puneet6060_0_6', 'khu-deep-learning-competition_0_10', 'kunstmatigeintelligentie20192020_0_5', 'labeled-surgical-tools_dilavado_0_4', 'land-cover-class_0_10', 'lego-brick-images_joosthazelzet_0_16', 'lego-brick-sorting-image-recognition_pacogarciam3_0_20', 'lego-minifigures-classification_ihelon_0_14', 'lego-vs-generic-brick-image-recognition_pacogarciam3_0_4', 'make-up-vs-no-make-up_petersunga_0_2', 'malefemale-for-drr_0_2', 'messy-vs-clean-room_cdawn1_0_2', 'microsoft-catsvsdogs-dataset_shaunthesheep_0_2', 'mis583-hw2-part-2_0_5', 'mllabgame_0_10', 'mushrooms-classification-common-genuss-images_maysee_0_9', 'mvtec_anomaly_detection_carpet_0_6', 'mvtec_anomaly_detection_grid_0_6', 'mvtec_anomaly_detection_leather_0_6', 'nnfl-cnn-lab2_0_6', 'notmnist_jwjohnson314_0_10', 'numta_BengaliAI_0_10', 'nuu-me-midterm-exam-image-classification_0_5', 'oregon-wildlife_virtualdvid_0_20', 'parkinsons-drawings_kmader_0_2', 'perritos_0_10', 'plant-seedlings-classification_0_12', 'proptit-aif-homework-1_0_8', 'real-and-fake-face-detection_ciplab_0_2', 'real-life-industrial-dataset-of-casting-product_ravirajsinh45_0_2', 'rockpaperscissors_drgfreeman_0_3', 'sample_nih-chest-xrays_0_2', 'sfu-cmpt-computer-vision-course-cnn_0_20', 'sfu-cmpt-computer-vision-course-cnn_100_120', 'sfu-cmpt-computer-vision-course-cnn_120_140', 'sfu-cmpt-computer-vision-course-cnn_140_160', 'sfu-cmpt-computer-vision-course-cnn_160_180', 'sfu-cmpt-computer-vision-course-cnn_180_200', 'simpsons4_0_20', 'sfu-cmpt-computer-vision-course-cnn_20_40', 'sfu-cmpt-computer-vision-course-cnn_40_60', 'sfu-cmpt-computer-vision-course-cnn_60_80', 'sfu-cmpt-computer-vision-course-cnn_80_100', 'sheep-breed-classification_divyansh22_0_4', 'simpsons4_20_39', 'simpsons-challenge-gft_0_20', 'simpsons-challenge-gft_20_39', 'skin-cancer9-classesisic_nodoubttome_0_9', 'sldc_0_10', 'stanford-dogs-dataset_jessicali9530_0_20', 'stanford-dogs-dataset_jessicali9530_100_120', 'stanford-dogs-dataset_jessicali9530_20_40', 'stanford-dogs-dataset_jessicali9530_40_60', 'stanford-dogs-dataset_jessicali9530_60_80', 'stanford-dogs-dataset_jessicali9530_80_100', 'stanford-dogs-dataset-traintest_miljan_0_20', 'stanford-dogs-dataset-traintest_miljan_100_120', 'stanford-dogs-dataset-traintest_miljan_20_40', 'stanford-dogs-dataset-traintest_miljan_40_60', 'stanford-dogs-dataset-traintest_miljan_60_80', 'stanford-dogs-dataset-traintest_miljan_80_100', 'synthetic-digits_prasunroy_0_10', 'tau-ethiopic-digit-recognition_0_10', 'the-simpsons-characters-dataset_alexattia_0_20', 'the-simpsons-characters-dataset_alexattia_20_39', 'tl-signs-hse-itmo-2020-winter_0_17', 'tl-signs-hse-itmo-2020-winter_17_34', 'tl-signs-hse-itmo-2020-winter_34_51', 'tl-signs-hse-itmo-2020-winter_51_67', 'vehicle_0_17', 'zalando-store-crawl_dqmonn_0_6'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['task', 'clss', 'nclss', 'x_query_train', 'y_query_train', 'x_query_test', 'y_query_test'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files['6000-store-items-images-classified-by-color_imoore_0_12'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(data_files['stanford-dogs-dataset_jessicali9530_40_60']['x_query_train']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(data_files['6000-store-items-images-classified-by-color_imoore_0_12']['x_query_test']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30533/3022977385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ImageCHD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_dict' is not defined"
     ]
    }
   ],
   "source": [
    "data_dict['ImageCHD'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageCHD 20\n",
      "MosMed 20\n",
      "kits 20\n",
      "pc 20\n",
      "RSPECT 20\n",
      "chex 20\n",
      "IHD_Brain 20\n",
      "Brain_MRI 20\n",
      "CTPancreas 20\n",
      "nih 20\n",
      "ProstateMRI 20\n",
      "LiTs 20\n"
     ]
    }
   ],
   "source": [
    "for key, value in data_dict.items():\n",
    "    print(key, len(value['x_query_train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Save the data_train dictionary as pt file\n",
    "#torch.save(data_dict, '/nfs/projects/mbzuai/shikhar/datasets/ofa/our_m_train.pt')\n",
    "torch.save(data_dict,'/nfs/projects/mbzuai/shikhar/datasets/ofa/our_data_path/meta_train.pt' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torchxrayvision as xrv\n",
    "nih_path = '/nfs/projects/mbzuai/shikhar/datasets/nih/images'\n",
    "chexpert_path = '/nfs/projects/mbzuai/shikhar/datasets/chexpert_small/CheXpert-v1.0-small'\n",
    "padchest_path = '/nfs/projects/mbzuai/shikhar/datasets/padchest'\n",
    "mimic_path = '/nfs/projects/mbzuai/shikhar/datasets/mimic/physionet.org/files/mimic-cxr-jpg/2.0.0'\n",
    "dataset = xrv.datasets.MIMIC_Dataset(\n",
    "                imgpath=mimic_path + '/files',\n",
    "                csvpath=mimic_path +'/mimic-cxr-2.0.0-metadata.csv.gz',\n",
    "                metacsvpath=mimic_path +'/mimic-cxr-2.0.0-chexpert.csv.gz',\n",
    "                transform=None, data_aug=None, unique_patients=False)\n",
    "dataset.pathologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.load('/nfs/users/ext_shikhar.srivastava/workspace/TANS/outcomes/ours/20220226_1107/retrieval/retrieval.pt', map_location = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'transfer.csv'\n",
    "import pandas as pd\n",
    "transfer = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source_model_path</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>target_dataset</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>ProstateMRI</td>\n",
       "      <td>kits</td>\n",
       "      <td>0.484182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>Brain_MRI</td>\n",
       "      <td>IHD_Brain</td>\n",
       "      <td>0.707317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>IHD_Brain</td>\n",
       "      <td>Brain_MRI</td>\n",
       "      <td>0.872581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>Brain_MRI</td>\n",
       "      <td>RSPECT</td>\n",
       "      <td>0.775966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>kits</td>\n",
       "      <td>RSPECT</td>\n",
       "      <td>0.803954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>572</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>ProstateMRI</td>\n",
       "      <td>IHD_Brain</td>\n",
       "      <td>0.759941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>573</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>ProstateMRI</td>\n",
       "      <td>IHD_Brain</td>\n",
       "      <td>0.759878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>574</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>ProstateMRI</td>\n",
       "      <td>IHD_Brain</td>\n",
       "      <td>0.759146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>575</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>ProstateMRI</td>\n",
       "      <td>IHD_Brain</td>\n",
       "      <td>0.769010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>576</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>ProstateMRI</td>\n",
       "      <td>IHD_Brain</td>\n",
       "      <td>0.768345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                  source_model_path  \\\n",
       "0             0  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...   \n",
       "1             1  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...   \n",
       "2             2  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...   \n",
       "3             3  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...   \n",
       "4             4  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...   \n",
       "..          ...                                                ...   \n",
       "572         572  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...   \n",
       "573         573  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...   \n",
       "574         574  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...   \n",
       "575         575  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...   \n",
       "576         576  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...   \n",
       "\n",
       "    source_dataset target_dataset        f1  \n",
       "0      ProstateMRI           kits  0.484182  \n",
       "1        Brain_MRI      IHD_Brain  0.707317  \n",
       "2        IHD_Brain      Brain_MRI  0.872581  \n",
       "3        Brain_MRI         RSPECT  0.775966  \n",
       "4             kits         RSPECT  0.803954  \n",
       "..             ...            ...       ...  \n",
       "572    ProstateMRI      IHD_Brain  0.759941  \n",
       "573    ProstateMRI      IHD_Brain  0.759878  \n",
       "574    ProstateMRI      IHD_Brain  0.759146  \n",
       "575    ProstateMRI      IHD_Brain  0.769010  \n",
       "576    ProstateMRI      IHD_Brain  0.768345  \n",
       "\n",
       "[577 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.stack(data_dict['MosMed']['x_query_train']).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "# Read pt file from path \n",
    "#path = '/l/users/shikhar.srivastava/data/ofa/www.dropbox.com/s/1qbsgjqanxgw2ji/p_m_train.pt'\n",
    "path ='/nfs/projects/mbzuai/shikhar/datasets/ofa/p_mod_zoo.pt' #p_mod_zoo.pt\n",
    "model_files = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset', 'acc', 'n_params', 'topol', 'f_emb'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['best_epoch', 'f1', 'loss', 'model_path', 'dataset', 'model', 'with_aug', 'imagenet_pretrained', 'f_emb', 'acc', 'topn'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict['f_emb'][10].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Model Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Xrays!\n",
      "=====================\n",
      "Sanity Model Parse\n",
      "Unknowns:  0\n",
      "Empty DataFrame\n",
      "Columns: [best_epoch, f1, loss, model_path, dataset, model, with_aug, balanced, pretrained, batch_128, topn]\n",
      "Index: []\n",
      "=====================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32063/52133006.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_model_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_and_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_model_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/TANS/meta/embedding.py\u001b[0m in \u001b[0;36mparse_and_embed\u001b[0;34m(self, standard_size, no_xray)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmeta_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataParallel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ofa/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ofa/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ofa/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ofa/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from meta.embedding import ModelEmbeddings\n",
    "model_embed = ModelEmbeddings()\n",
    "_model_dict = model_embed.parse_and_embed()\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(_model_dict)\n",
    "model_dict = df.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model_dict, '/nfs/projects/mbzuai/shikhar/datasets/ofa/our_mod_zoo.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "model_dict = torch.load('/nfs/projects/mbzuai/shikhar/datasets/ofa/our_mod_zoo.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Brain_MRI',\n",
       " 'CTPancreas',\n",
       " 'Covid19XRay',\n",
       " 'IHD_Brain',\n",
       " 'ImageCHD',\n",
       " 'LiTs',\n",
       " 'MosMed',\n",
       " 'ProstateMRI',\n",
       " 'RSNAXRay',\n",
       " 'RSPECT',\n",
       " 'kits'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "set(pd.DataFrame(model_dict['dataset'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28827/2234684669.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/nfs/projects/mbzuai/shikhar/datasets/ofa/our_mod_zoo.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_dict' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['best_epoch', 'acc', 'f1', 'loss', 'model_path', 'dataset', 'model', 'with_aug', 'balanced', 'pretrained', 'batch_128', 'topn', 'f_emb'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict['f_emb'][10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "cross_modal_info = torch.load('/nfs/users/ext_shikhar.srivastava/workspace/TANS/outcomes/ours/20220226_1921/retrieval/retrieval.pt', map_location = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cross_modal_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(cross_modal_info['dataset'] == 'IHD_Brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28827/563011116.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_modal_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "torch.tensor(cross_modal_info['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28827/1187685944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_modal_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'IHD_Brain'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "torch.where(torch.tensor(cross_modal_info['dataset']) == 'IHD_Brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   5,   8,  10,  12,  15,  17,  18,  20,  21,  23,  24,\n",
       "         26,  28,  30,  31,  32,  34,  36,  37,  39,  43,  44,  50,  68,\n",
       "         69,  73,  76,  77,  81,  84,  86,  94,  98,  99, 101, 106, 107,\n",
       "        108, 109, 112, 117, 119, 125, 127, 132, 137, 138, 148, 152, 154,\n",
       "        158, 159, 163, 167, 183, 186, 196, 197, 207, 214, 232]),)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(cross_modal_info['dataset']) == 'IHD_Brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset', 'm_emb', 'acc', 'f1', 'best_epoch', 'loss', 'model_path', 'model', 'with_aug', 'balanced', 'pretrained', 'batch_128', 'topn'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_modal_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.stack(cross_modal_info['m_emb']).shape\n",
    "# mask tensor at indices mask_indices\n",
    "def mask_tensor(tensor, mask_indices):\n",
    "    mask = torch.ones_like(tensor)\n",
    "    mask[mask_indices] = 0\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask_tensor(torch.stack(cross_modal_info['m_emb']), mask_indices)\n",
    "# mask cross_modal_info at indices mask_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.stack(cross_modal_info['m_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0595,  0.1067,  0.0020, -0.1429, -0.1218,  0.0984, -0.0917,  0.0712,\n",
       "          0.0487, -0.0549, -0.1822,  0.0372, -0.0516,  0.1076, -0.1557, -0.0383,\n",
       "         -0.0738,  0.0534,  0.1010, -0.1133,  0.0291, -0.0498,  0.0387, -0.1476,\n",
       "         -0.1285, -0.0363, -0.1401, -0.0962,  0.0104,  0.0680, -0.0879,  0.0533,\n",
       "          0.0102, -0.0610,  0.0668,  0.0754,  0.0983, -0.1319, -0.0657,  0.0508,\n",
       "          0.0737,  0.0779,  0.0774,  0.0923,  0.0499,  0.0253, -0.0462, -0.1058,\n",
       "         -0.1247, -0.0700,  0.0575, -0.1216,  0.1344, -0.1073,  0.0534, -0.0977,\n",
       "          0.0003, -0.0989, -0.0672, -0.0538, -0.1963,  0.0348, -0.0940,  0.0335,\n",
       "         -0.2159,  0.0935, -0.1875,  0.0698, -0.1239, -0.0825,  0.0556, -0.1105,\n",
       "          0.0958,  0.1022, -0.0494,  0.0327,  0.1173,  0.1277,  0.0768,  0.0953,\n",
       "         -0.0510,  0.0552,  0.0474,  0.0892, -0.0656, -0.0533,  0.0411, -0.1527,\n",
       "         -0.1136, -0.0525, -0.0465,  0.0500, -0.0659,  0.0899, -0.0558, -0.0581,\n",
       "          0.1385,  0.0663, -0.0265,  0.0580, -0.0804,  0.0810, -0.0789,  0.0503,\n",
       "          0.1303,  0.0442, -0.0958, -0.0557,  0.0909,  0.0314,  0.1263,  0.1142,\n",
       "         -0.0778, -0.0784,  0.0455, -0.0043,  0.0830, -0.1484, -0.0445, -0.0308,\n",
       "          0.0712,  0.0310,  0.0345, -0.1211,  0.0659,  0.0893,  0.0346, -0.0177]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_indices = [i for i,x  in enumerate(cross_modal_info['dataset']) if x == 'IHD_Brain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[mask_indices] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.0595,  0.1067,  0.0020,  ...,  0.0893,  0.0346, -0.0177]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1348,  0.0025,  0.0129,  ...,  0.0250,  0.0254,  0.0627]],\n",
       "\n",
       "        [[ 0.0889,  0.0723,  0.0426,  ...,  0.0731,  0.0461, -0.1219]],\n",
       "\n",
       "        [[-0.0757,  0.1311, -0.0548,  ..., -0.0920,  0.0773,  0.0949]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "~ (operator.invert) is only implemented on integer and Boolean-type tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9684/2938341508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ~ (operator.invert) is only implemented on integer and Boolean-type tensors"
     ]
    }
   ],
   "source": [
    "# invert mask to get indices to be masked\n",
    "mask_indices = torch.nonzero(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9684/2799209274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "a * (1-mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>m_emb</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>model_path</th>\n",
       "      <th>model</th>\n",
       "      <th>with_aug</th>\n",
       "      <th>balanced</th>\n",
       "      <th>pretrained</th>\n",
       "      <th>batch_128</th>\n",
       "      <th>topn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IHD_Brain</td>\n",
       "      <td>[[tensor(0.1089), tensor(0.1640), tensor(0.086...</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>46</td>\n",
       "      <td>0.278173</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>top-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IHD_Brain</td>\n",
       "      <td>[[tensor(0.1599), tensor(0.1391), tensor(0.090...</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>97</td>\n",
       "      <td>0.439231</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>top-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RSPECT</td>\n",
       "      <td>[[tensor(-0.0595), tensor(0.1067), tensor(0.00...</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>11</td>\n",
       "      <td>0.507455</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>top-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RSPECT</td>\n",
       "      <td>[[tensor(-0.1280), tensor(0.0744), tensor(-0.0...</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.235808</td>\n",
       "      <td>16</td>\n",
       "      <td>0.549068</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>mobilenet_v2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>top-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RSPECT</td>\n",
       "      <td>[[tensor(-0.0489), tensor(0.1197), tensor(0.08...</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>95</td>\n",
       "      <td>0.150664</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>top-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>ImageCHD</td>\n",
       "      <td>[[tensor(-0.0664), tensor(0.0455), tensor(-0.0...</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.188811</td>\n",
       "      <td>22</td>\n",
       "      <td>0.089775</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>mobilenet_v2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>Brain_MRI</td>\n",
       "      <td>[[tensor(0.0341), tensor(0.1585), tensor(0.020...</td>\n",
       "      <td>0.932258</td>\n",
       "      <td>0.932258</td>\n",
       "      <td>100</td>\n",
       "      <td>0.013777</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>MosMed</td>\n",
       "      <td>[[tensor(-0.1348), tensor(0.0025), tensor(0.01...</td>\n",
       "      <td>0.759375</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>44</td>\n",
       "      <td>0.143112</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>kits</td>\n",
       "      <td>[[tensor(0.0889), tensor(0.0723), tensor(0.042...</td>\n",
       "      <td>0.859747</td>\n",
       "      <td>0.530511</td>\n",
       "      <td>85</td>\n",
       "      <td>0.094970</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>ProstateMRI</td>\n",
       "      <td>[[tensor(-0.0757), tensor(0.1311), tensor(-0.0...</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>0.594406</td>\n",
       "      <td>35</td>\n",
       "      <td>0.471072</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset                                              m_emb       acc  \\\n",
       "0      IHD_Brain  [[tensor(0.1089), tensor(0.1640), tensor(0.086...  0.826667   \n",
       "1      IHD_Brain  [[tensor(0.1599), tensor(0.1391), tensor(0.090...  0.790000   \n",
       "2         RSPECT  [[tensor(-0.0595), tensor(0.1067), tensor(0.00...  0.776667   \n",
       "3         RSPECT  [[tensor(-0.1280), tensor(0.0744), tensor(-0.0...  0.708333   \n",
       "4         RSPECT  [[tensor(-0.0489), tensor(0.1197), tensor(0.08...  0.950000   \n",
       "..           ...                                                ...       ...   \n",
       "375     ImageCHD  [[tensor(-0.0664), tensor(0.0455), tensor(-0.0...  0.818750   \n",
       "376    Brain_MRI  [[tensor(0.0341), tensor(0.1585), tensor(0.020...  0.932258   \n",
       "377       MosMed  [[tensor(-0.1348), tensor(0.0025), tensor(0.01...  0.759375   \n",
       "378         kits  [[tensor(0.0889), tensor(0.0723), tensor(0.042...  0.859747   \n",
       "379  ProstateMRI  [[tensor(-0.0757), tensor(0.1311), tensor(-0.0...  0.773438   \n",
       "\n",
       "           f1  best_epoch      loss  \\\n",
       "0    0.452632          46  0.278173   \n",
       "1    0.790698          97  0.439231   \n",
       "2    0.263736          11  0.507455   \n",
       "3    0.235808          16  0.549068   \n",
       "4    0.210526          95  0.150664   \n",
       "..        ...         ...       ...   \n",
       "375  0.188811          22  0.089775   \n",
       "376  0.932258         100  0.013777   \n",
       "377  0.702703          44  0.143112   \n",
       "378  0.530511          85  0.094970   \n",
       "379  0.594406          35  0.471072   \n",
       "\n",
       "                                            model_path         model  \\\n",
       "0    /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...      resnet18   \n",
       "1    /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...      resnet50   \n",
       "2    /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...      resnet18   \n",
       "3    /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...  mobilenet_v2   \n",
       "4    /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...      resnet50   \n",
       "..                                                 ...           ...   \n",
       "375  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...  mobilenet_v2   \n",
       "376  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...      resnet18   \n",
       "377  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...      resnet18   \n",
       "378  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...      resnet50   \n",
       "379  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...      resnet50   \n",
       "\n",
       "     with_aug  balanced  pretrained  batch_128   topn  \n",
       "0       False     False       False      False  top-2  \n",
       "1        True      True       False       True  top-2  \n",
       "2        True     False        True       True  top-2  \n",
       "3        True     False       False       True  top-2  \n",
       "4        True     False       False      False  top-2  \n",
       "..        ...       ...         ...        ...    ...  \n",
       "375      True     False       False      False    raw  \n",
       "376      True      True        True       True    raw  \n",
       "377      True     False       False      False    raw  \n",
       "378      True      True       False       True    raw  \n",
       "379      True     False       False       True    raw  \n",
       "\n",
       "[380 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cross_modal_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "for row in df['f_emb']:\n",
    "    clear_output(wait = False)\n",
    "    print(row.numpy().shape)\n",
    "    print(list(row.numpy()))\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>loss</th>\n",
       "      <th>model_path</th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>with_aug</th>\n",
       "      <th>balanced</th>\n",
       "      <th>topn</th>\n",
       "      <th>f_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>0.278173</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>IHD_Brain</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>top-2</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>0.507455</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>RSPECT</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>top-2</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.235808</td>\n",
       "      <td>0.549068</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>RSPECT</td>\n",
       "      <td>mobilenet_v2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>top-2</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.150664</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>RSPECT</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>top-2</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.827692</td>\n",
       "      <td>0.451216</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>RSPECT</td>\n",
       "      <td>mobilenet_v2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>top-2</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>9</td>\n",
       "      <td>0.819531</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.442918</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>ImageCHD</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>raw</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(41.2831), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>42</td>\n",
       "      <td>0.855469</td>\n",
       "      <td>0.780415</td>\n",
       "      <td>0.259087</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>CTPancreas</td>\n",
       "      <td>efficientnet_b4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>raw</td>\n",
       "      <td>[tensor(0.0308), tensor(-0.0279), tensor(-0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>22</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.188811</td>\n",
       "      <td>0.089775</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>ImageCHD</td>\n",
       "      <td>mobilenet_v2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>raw</td>\n",
       "      <td>[tensor(2.0657), tensor(2.1143), tensor(1.8511...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>44</td>\n",
       "      <td>0.759375</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.143112</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>MosMed</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>raw</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>35</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>0.594406</td>\n",
       "      <td>0.471072</td>\n",
       "      <td>/nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...</td>\n",
       "      <td>ProstateMRI</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>raw</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     best_epoch       acc        f1      loss  \\\n",
       "0            46  0.826667  0.452632  0.278173   \n",
       "1            11  0.776667  0.263736  0.507455   \n",
       "2            16  0.708333  0.235808  0.549068   \n",
       "3            95  0.950000  0.210526  0.150664   \n",
       "4            18  0.813333  0.827692  0.451216   \n",
       "..          ...       ...       ...       ...   \n",
       "295           9  0.819531  0.094118  0.442918   \n",
       "296          42  0.855469  0.780415  0.259087   \n",
       "297          22  0.818750  0.188811  0.089775   \n",
       "298          44  0.759375  0.702703  0.143112   \n",
       "299          35  0.773438  0.594406  0.471072   \n",
       "\n",
       "                                            model_path      dataset  \\\n",
       "0    /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...    IHD_Brain   \n",
       "1    /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...       RSPECT   \n",
       "2    /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...       RSPECT   \n",
       "3    /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...       RSPECT   \n",
       "4    /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...       RSPECT   \n",
       "..                                                 ...          ...   \n",
       "295  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...     ImageCHD   \n",
       "296  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...   CTPancreas   \n",
       "297  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...     ImageCHD   \n",
       "298  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...       MosMed   \n",
       "299  /nfs/projects/mbzuai/BioMedIA/MICCIA_22/logs/c...  ProstateMRI   \n",
       "\n",
       "               model  with_aug  balanced   topn  \\\n",
       "0           resnet18     False     False  top-2   \n",
       "1           resnet18      True     False  top-2   \n",
       "2       mobilenet_v2      True     False  top-2   \n",
       "3           resnet50      True     False  top-2   \n",
       "4       mobilenet_v2      True      True  top-2   \n",
       "..               ...       ...       ...    ...   \n",
       "295         resnet50      True     False    raw   \n",
       "296  efficientnet_b4      True     False    raw   \n",
       "297     mobilenet_v2      True     False    raw   \n",
       "298         resnet18      True     False    raw   \n",
       "299         resnet50      True     False    raw   \n",
       "\n",
       "                                                 f_emb  \n",
       "0    [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "1    [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "2    [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "3    [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "4    [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "..                                                 ...  \n",
       "295  [tensor(0.), tensor(0.), tensor(41.2831), tens...  \n",
       "296  [tensor(0.0308), tensor(-0.0279), tensor(-0.01...  \n",
       "297  [tensor(2.0657), tensor(2.1143), tensor(1.8511...  \n",
       "298  [tensor(0.), tensor(0.), tensor(0.), tensor(2....  \n",
       "299  [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "\n",
       "[300 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import clear_output\n",
    "for row in df['x_query_train'][0]:\n",
    "    print(row.numpy().shape)\n",
    "    print(list(row.numpy()))\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38470bdf3e966986ca85f0825073bcf86ddf5dbb9002a7f4b160e6f6a6aff5a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ofa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
